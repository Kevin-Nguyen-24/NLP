{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SPdmy5uha1PZ","executionInfo":{"status":"ok","timestamp":1753639485171,"user_tz":240,"elapsed":19189,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"a47efbc2-5efe-4c48-867b-6519daac0c61"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# **Introduction**\n","In this project, we perform sentiment analysis on the IMDB Movie Reviews dataset, which contains 50,000 movie reviews labeled as either positive or negative. The dataset is publicly available at:\n","http://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","We fine-tune the pretrained Transformer model distilbert-base-uncased for this binary classification task. DistilBERT is a compressed version of BERT that retains over 95% of BERTâ€™s accuracy, while being significantly more efficient â€” 40% smaller and 60% faster in inference.\n","\n","Unlike BERT, which is computationally heavier and slower to train, DistilBERT provides a practical trade-off between performance and efficiency. This makes it especially useful for real-world applications where speed and resource constraints matter.\n","\n","We tokenize input reviews using the DistilBERT tokenizer with truncation and padding, and then fine-tune the model using standard classification techniques. The model is evaluated using accuracy and F1-score to measure its performance in predicting the sentiment of unseen reviews."],"metadata":{"id":"GaghOHxLDSsi"}},{"cell_type":"code","source":["# ===============================\n","# 1. Install Required Libraries\n","# ===============================\n","!pip install transformers evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xq6N0PEJsXTI","executionInfo":{"status":"ok","timestamp":1753635799987,"user_tz":240,"elapsed":14178,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"203ecf4e-0063-4518-bf43-13a078e053f2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.53.3)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.7.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 2. Load Dataset\n","# ===============================\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","# Load IMDB CSV (make sure path is correct or upload)\n","df = pd.read_csv(\"/content/drive/MyDrive/DL/project3/IMDB Dataset.csv\")\n","df = df.sample(n=2000, random_state=42).reset_index(drop=True)\n","df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n","\n","# Split\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    df['review'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",")"],"metadata":{"id":"S7SpPLV5scuS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 3. Tokenize with BERT Tokenizer\n","# ===============================\n","from transformers import AutoTokenizer\n","\n","# You can change this to 'distilbert-base-uncased' or 'bert-base-uncased'\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","\n","# Tokenize the dataset\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cH-_uaTase7-","executionInfo":{"status":"ok","timestamp":1753635854588,"user_tz":240,"elapsed":17649,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"d02d8717-0d54-4208-bfb3-3034fa695925"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 4. Create Torch Dataset\n","# ===============================\n","import torch\n","\n","class IMDBDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","train_dataset = IMDBDataset(train_encodings, train_labels)\n","val_dataset = IMDBDataset(val_encodings, val_labels)"],"metadata":{"id":"_eWQ5PsSshWb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import DistilBertForSequenceClassification\n","\n","model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G3dx0cgTslMP","executionInfo":{"status":"ok","timestamp":1753635871233,"user_tz":240,"elapsed":6936,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"095cdd4a-0cc9-448f-ee8a-edea9c853bcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 6. Training Configuration\n","# ===============================\n","from transformers import TrainingArguments, Trainer\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./distilbert-base-uncased-imdb\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    logging_dir=\"./logs\",\n","    report_to=\"none\"\n",")"],"metadata":{"id":"N4lC_24Wsl7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 7. Define Metrics\n","# ===============================\n","import evaluate\n","\n","accuracy = evaluate.load(\"accuracy\")\n","f1 = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = logits.argmax(axis=-1)\n","    return {\n","        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n","        \"f1\": f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n","    }"],"metadata":{"id":"1uqeV27Xsoay"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# ===============================\n","# 8. Train the Model\n","# ===============================\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":238},"id":"EVgIXZRMsqcA","executionInfo":{"status":"ok","timestamp":1753635963969,"user_tz":240,"elapsed":82388,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"2dd14265-42cc-400f-eb7c-d96fdec9f5ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-9-4005980745.py:4: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [150/150 01:19, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.578100</td>\n","      <td>0.365769</td>\n","      <td>0.845000</td>\n","      <td>0.845295</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.281400</td>\n","      <td>0.418856</td>\n","      <td>0.810000</td>\n","      <td>0.801918</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.131900</td>\n","      <td>0.390446</td>\n","      <td>0.850000</td>\n","      <td>0.850501</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=150, training_loss=0.3304743258158366, metrics={'train_runtime': 80.4009, 'train_samples_per_second': 29.85, 'train_steps_per_second': 1.866, 'total_flos': 158960878387200.0, 'train_loss': 0.3304743258158366, 'epoch': 3.0})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["\n","# ===============================\n","# 9. Evaluate the Model\n","# ===============================\n","results = trainer.evaluate()\n","print(\"\\nðŸ“Š Final Evaluation:\")\n","for k, v in results.items():\n","    print(f\"{k}: {v:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"J1CWgpydssXd","executionInfo":{"status":"ok","timestamp":1753635982127,"user_tz":240,"elapsed":1418,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"5198b079-c028-4d04-f754-69de902d50a3"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [13/13 00:01]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Final Evaluation:\n","eval_loss: 0.3658\n","eval_accuracy: 0.8450\n","eval_f1: 0.8453\n","eval_runtime: 1.4134\n","eval_samples_per_second: 141.5040\n","eval_steps_per_second: 9.1980\n","epoch: 3.0000\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 10. Predict Sample\n","# ===============================\n","def predict(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to the same device as the model\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        probs = torch.softmax(outputs.logits, dim=-1)\n","        predicted_class = torch.argmax(probs).item()\n","    return \"positive\" if predicted_class == 1 else \"negative\", probs[0].tolist()\n","\n","# Example prediction\n","print(\"\\nðŸ§ª Example Prediction:\")\n","example_text = \"this man .\"\n","pred, prob = predict(example_text)\n","print(f\"Text: {example_text}\\nPrediction: {pred}, Probabilities: {prob}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ugg00sIssucU","executionInfo":{"status":"ok","timestamp":1753635984433,"user_tz":240,"elapsed":74,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"a97e42f0-d4bd-405c-9965-ecf108de6757"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ§ª Example Prediction:\n","Text: this man .\n","Prediction: positive, Probabilities: [0.3708809018135071, 0.6291190385818481]\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}