COâ‚‚ Time Series Forecasting with LSTM
This project demonstrates forecasting future COâ‚‚ concentration levels using a Long Short-Term Memory (LSTM) neural network in PyTorch.

It includes data normalization, sequence generation for supervised learning, model training, validation, and visualization.

ğŸ“Œ Features
Load and visualize raw COâ‚‚ time series.

Normalize data to [-1, 1].

Create input-output sequences for prediction.

Train/validation/test split.

PyTorch Dataset & DataLoader integration.

Multi-layer LSTM model.

Training with MSE loss.

TensorBoard logging.

Plot training/validation loss over epochs.

ğŸ—‚ï¸ Project Structure
css
Copy
Edit
.
â”œâ”€â”€ data/
â”‚   â””â”€â”€ co2.csv
â”œâ”€â”€ models/
â”‚   â””â”€â”€ lstm_model.pt (optional, saved model)
â”œâ”€â”€ runs/
â”‚   â””â”€â”€ co2_experiment/ (TensorBoard logs)
â”œâ”€â”€ src/
â”‚   â””â”€â”€ main.py
â””â”€â”€ README.md
ğŸ“ˆ Example Plot
Example of the loss curve:


ğŸ§ª Setup Instructions
1ï¸âƒ£ Install Dependencies
bash
Copy
Edit
pip install numpy pandas matplotlib scikit-learn torch tensorboard
2ï¸âƒ£ Prepare Data
Place your COâ‚‚ time series CSV in the data/ folder.

Example CSV format:

time	co2
1958-03-01	315.7
1958-04-01	317.6
...	...

ğŸ—ï¸ Usage
Run training:

bash
Copy
Edit
python src/main.py
Monitor TensorBoard logs:

bash
Copy
Edit
tensorboard --logdir=runs
âš™ï¸ Main Parameters
You can adjust these in the script:

Parameter	Description	Example
INPUT_LEN	Input sequence length	12
OUTPUT_LEN	Output prediction length	1
HIDDEN_SIZE	LSTM hidden units	100
NUM_LAYERS	Number of LSTM layers	2
BATCH_SIZE	Training batch size	64
EPOCHS	Number of training epochs	20
DEVICE	'cuda' or 'cpu'	'cuda'

ğŸ” What Does It Do?
âœ” Loads and visualizes COâ‚‚ time series.
âœ” Normalizes data.
âœ” Converts it to sliding window sequences.
âœ” Splits data into train/val/test sets.
âœ” Defines and trains an LSTM forecasting model.
âœ” Logs training and validation loss to TensorBoard.
âœ” Plots loss curves.

ğŸ¤– Model Architecture
PyTorch LSTM

Input: past COâ‚‚ readings

Output: predicted future COâ‚‚ readings

Layers:

LSTM (multi-layer)

Linear decoder


