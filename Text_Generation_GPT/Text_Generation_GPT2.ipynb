{"cells":[{"cell_type":"code","execution_count":null,"id":"78528b0e-2f01-4c35-abe1-897863d3498b","metadata":{"id":"78528b0e-2f01-4c35-abe1-897863d3498b"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["# **Introduction**\n","This project focuses on text generation using the GPT-2 (Generative Pretrained Transformer 2) model. GPT-2 is a powerful autoregressive Transformer developed by OpenAI, trained to predict the next word in a sequence based on previous context. It is widely used for tasks such as story generation, dialogue creation, and code completion.\n","\n","We fine-tune GPT-2 on a custom text corpus consisting of literary works (e.g., Moby Dick, Hamlet) to teach the model domain-specific language patterns. Fine-tuning a pretrained language model allows it to adapt to new styles or topics without training from scratch, saving both time and resources.\n","\n","After training, we use the model to generate coherent and creative text by providing it with prompts. The quality of the generated text is evaluated using common automatic metrics such as ROUGE, BLEU, and METEOR, which compare the generated outputs to reference texts based on word overlap and semantic similarity.\n","\n","GPT-2 is chosen for this task due to its strong generative capabilities, open accessibility, and support from the Hugging Face Transformers library, which simplifies model loading, tokenization, and training.\n","\n"],"metadata":{"id":"vH8eHyQiFhdJ"},"id":"vH8eHyQiFhdJ"},{"cell_type":"code","source":["# ===============================\n","# Load Dataset\n","# ===============================\n","import nltk\n","nltk.download('gutenberg')\n","from nltk.corpus import gutenberg\n","\n","selected_fileids = ['melville-moby_dick.txt' , 'shakespeare-hamlet.txt']  # Example: just 2 books\n","texts = [gutenberg.raw(fileid) for fileid in selected_fileids]\n","\n","full_corpus = \"\\n\".join(texts)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vacT3ESP-fdM","executionInfo":{"status":"ok","timestamp":1753631559230,"user_tz":240,"elapsed":5232,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"31b6083c-82ed-432e-cb2e-94f2764f8460","collapsed":true},"id":"vacT3ESP-fdM","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]   Package gutenberg is already up-to-date!\n"]}]},{"cell_type":"code","source":["# ===============================\n","# Preprocessing\n","# ===============================\n","import re\n","\n","def clean_text(text):\n","    text = text.lower()\n","    text = re.sub(r'\\s+', ' ', text)\n","    text = re.sub(r'[^a-zA-Z0-9\\s\\.\\,\\;\\:\\!\\?\\'\\\"]', '', text)\n","    text = text.strip()\n","    return text\n","\n","clean_corpus = clean_text(full_corpus)\n"],"metadata":{"id":"5jXoXKOf-zEP"},"id":"5jXoXKOf-zEP","execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# Pre_trained Model\n","# ===============================\n","from transformers import AutoTokenizer, AutoModelForCausalLM , Trainer, TrainingArguments, TextDataset, DataCollatorForLanguageModeling\n","\n","model_name=\"gpt2\"\n","\n","tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n","model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n","\n","\n","# Save cleaned text to file\n","with open(\"corpus.txt\", \"w\") as f:\n","    f.write(clean_corpus)\n","\n","# Create dataset\n","def load_dataset(file_path, tokenizer, block_size=128):\n","    return TextDataset(\n","        tokenizer=tokenizer,\n","        file_path=file_path,\n","        block_size=block_size,\n","        overwrite_cache=True\n","    )\n","\n","train_dataset = load_dataset(\"corpus.txt\", tokenizer)\n","\n","data_collator = DataCollatorForLanguageModeling(\n","    tokenizer=tokenizer,\n","    mlm=False\n",")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dABIfwQIvb8A","executionInfo":{"status":"ok","timestamp":1753631598304,"user_tz":240,"elapsed":33470,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"38e97eb9-6312-4156-9bd7-bb670a9bad3d","collapsed":true},"id":"dABIfwQIvb8A","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/transformers/data/datasets/language_modeling.py:53: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n","  warnings.warn(\n","Token indices sequence length is longer than the specified maximum sequence length for this model (329618 > 1024). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["# ===============================\n","# Train\n","# ===============================\n","import os\n","os.environ[\"WANDB_DISABLED\"] = \"true\"\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./gpt2-finetuned\",\n","    overwrite_output_dir=True,\n","    num_train_epochs=8,\n","    per_device_train_batch_size=16,\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    logging_dir=\"./logs\",\n","    report_to=\"none\"\n",")\n","\n","\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    data_collator=data_collator,\n","    train_dataset=train_dataset\n",")\n","\n","trainer.train()\n","trainer.save_model(\"./gpt2-finetuned\")\n","tokenizer.save_pretrained(\"./gpt2-finetuned\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":447},"id":"S09T2VFwsCf4","executionInfo":{"status":"ok","timestamp":1753632672991,"user_tz":240,"elapsed":1053135,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"313c5907-4aa6-47f1-a523-5eb501a4a6da"},"id":"S09T2VFwsCf4","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1288' max='1288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1288/1288 17:16, Epoch 8/8]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>161</td>\n","      <td>4.411600</td>\n","    </tr>\n","    <tr>\n","      <td>322</td>\n","      <td>4.128800</td>\n","    </tr>\n","    <tr>\n","      <td>483</td>\n","      <td>3.994800</td>\n","    </tr>\n","    <tr>\n","      <td>644</td>\n","      <td>3.894400</td>\n","    </tr>\n","    <tr>\n","      <td>805</td>\n","      <td>3.814800</td>\n","    </tr>\n","    <tr>\n","      <td>966</td>\n","      <td>3.756400</td>\n","    </tr>\n","    <tr>\n","      <td>1127</td>\n","      <td>3.709500</td>\n","    </tr>\n","    <tr>\n","      <td>1288</td>\n","      <td>3.683700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["('./gpt2-finetuned/tokenizer_config.json',\n"," './gpt2-finetuned/special_tokens_map.json',\n"," './gpt2-finetuned/vocab.json',\n"," './gpt2-finetuned/merges.txt',\n"," './gpt2-finetuned/added_tokens.json',\n"," './gpt2-finetuned/tokenizer.json')"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["# ===============================\n","# Text Generation & Evaluation Prep\n","# ===============================\n","import torch\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel\n","\n","# Load fine-tuned model and tokenizer\n","model_path = \"./gpt2-finetuned\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n","model = GPT2LMHeadModel.from_pretrained(model_path)\n","\n","# Define multiple prompts\n","prompts = [\"Once upon a time\",\n","           \"To be or not to be\",\n","           \"The whale was nowhere to be seen\",\n","           \"The ship was ready to sail\",\n","           \"This is the king\",\n","           \"She loved him once.\",\n","           \"He is dead.\"]\n","references = [p.lower() for p in prompts]\n","hypotheses = []\n","\n","# Generate text for each prompt\n","for prompt in prompts:\n","    inputs = tokenizer(prompt, return_tensors=\"pt\")\n","    output_ids = model.generate(\n","        input_ids=inputs[\"input_ids\"],\n","        attention_mask=inputs[\"attention_mask\"], #tells the model which tokens to focus on (1 = keep, 0 = ignore).\n","        max_length=50,\n","        do_sample=True,\n","        temperature=0.9, # Controls randomness of token selection (used with sampling).\n","        top_k=50,\n","        top_p=0.95,\n","        repetition_penalty=1.2 # Penalizes repeating phrases or tokens.\n","    )\n","    generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n","    hypotheses.append(generated_text)\n","\n","# Show all outputs\n","print(\"\\nGenerated Texts:\")\n","for i, (prompt, output) in enumerate(zip(prompts, hypotheses)):\n","    print(f\"\\nPrompt {i+1}: {prompt}\\nOutput: {output}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WLrz3m8DYaJh","executionInfo":{"status":"ok","timestamp":1753632697929,"user_tz":240,"elapsed":19966,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"59d71bb6-d458-46af-db00-f1b80db7e2c3"},"id":"WLrz3m8DYaJh","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["\n","Generated Texts:\n","\n","Prompt 1: Once upon a time\n","Output: Once upon a time, when the white whale's tremendous jaws and teeth should be so closely joined in them with those of other sharks which he then feeds on; it was not till about eight or ten that such unnatural jointing took place. as for\n","\n","Prompt 2: To be or not to be\n","Output: To be or not to be, in a certain measure there are two main considerations involved here; both involving the consideration of this matter simultaneously. first: we must remember that all whales generally have their mouths cut into small incisionments below by other creatures\n","\n","Prompt 3: The whale was nowhere to be seen\n","Output: The whale was nowhere to be seen, nor for a time could he have been heard from. \"there are those among the whalemen who would rather not know their seamen than hear them speak out; and there is no way they can possibly\n","\n","Prompt 4: The ship was ready to sail\n","Output: The ship was ready to sail; and she had but a few short steps between when i caught sight of the mainmast, then at last we saw that there were two small men in each masthead. they seemed old spaniardsmen from an\n","\n","Prompt 5: This is the king\n","Output: This is the king. a good fellow; and an honourable man in arms, whose head of mast was hoisted high over my heads to look out upon his harpooneer shipmate.\" \"his name,\" said i as if it were\n","\n","Prompt 6: She loved him once.\n","Output: She loved him once. he had a peculiar, almost unctuous disposition for all things to the point of being altogether detached from other men; and i have long since seen it manifest in his profound concern towards me as an adult man: that is\n","\n","Prompt 7: He is dead.\n","Output: He is dead. and what's the matter, my lord? where he has been killed; there!\" \"he was murdered by moby dick for a whalebone.\" \"'tis all right with thee,' said i to queequeg: '\n"]}]},{"cell_type":"code","source":["# ===============================\n","# Evaluation\n","# ===============================\n","\n","!pip install rouge_score\n","!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kBILP2FjJYxM","executionInfo":{"status":"ok","timestamp":1753632715411,"user_tz":240,"elapsed":10638,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"1902bd55-dcb5-4b9e-a84a-4661f602c7c4","collapsed":true},"id":"kBILP2FjJYxM","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n","Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n","Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.5)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.0.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.33.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.14)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.14.1)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.7.14)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.6.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}]},{"cell_type":"code","source":["# ===============================\n","# We used three methods to evaluate our text generation model: ROUGE, BLEU, and METEOR.\n","\n","# ROUGE checks how many words or phrases match between the generated text and the reference.\n","\n","# BLEU looks for exact word matches and is good for translation tasks.\n","\n","# METEOR considers similar words, word order, and meaning, which makes it better for creative or open-ended text.\n","\n","# We chose to focus on METEOR because our model generates free-form sentences (like stories or literary lines), and METEOR gives a better idea of how close the meaning is to the original.\n","# ROUGE and BLEU are included for comparison, but METEOR is more useful for this kind of task.\n","\n","# ===============================\n","\n","\n","import evaluate\n","\n","rouge = evaluate.load(\"rouge\")\n","bleu = evaluate.load(\"bleu\")\n","meteor = evaluate.load(\"meteor\")\n","\n","rouge_result = rouge.compute(predictions=hypotheses, references=references)\n","print(\"ROUGE:\", rouge_result)\n","\n","# BLEU expects references as list of lists . We can skip this metric\n","# bleu_result = bleu.compute(predictions=hypotheses, references=[[ref] for ref in references])\n","# print(\"BLEU:\", bleu_result)\n","\n","\n","meteor_result = meteor.compute(predictions=hypotheses, references=references)\n","print(\"METEOR:\", meteor_result)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ppHq5zz1VgCo","executionInfo":{"status":"ok","timestamp":1753632776952,"user_tz":240,"elapsed":2874,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"222ad8b0-9ddb-440d-9942-4e0ffef50df6"},"id":"ppHq5zz1VgCo","execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Package punkt_tab is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]},{"output_type":"stream","name":"stdout","text":["ROUGE: {'rouge1': np.float64(0.20625850340136057), 'rouge2': np.float64(0.1697648624667258), 'rougeL': np.float64(0.206734693877551), 'rougeLsum': np.float64(0.20573267144695717)}\n","METEOR: {'meteor': np.float64(0.5046195773847437)}\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":5}