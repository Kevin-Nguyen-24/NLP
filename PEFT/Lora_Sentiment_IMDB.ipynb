{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"SPdmy5uha1PZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Introduction**\n","This project focuses on sentiment analysis using the IMDB Movie Reviews dataset, which contains 50,000 movie reviews evenly split between positive and negative sentiment labels. The dataset is available at:\n","http://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\n","\n","We fine-tune a pretrained Transformer model, specifically DistilBERT (distilbert-base-uncased), using a parameter-efficient method called LoRA (Low-Rank Adaptation). LoRA allows us to adapt large models efficiently by injecting small trainable rank matrices into attention layers, reducing computational cost."],"metadata":{"id":"iWMsrDOcA60n"}},{"cell_type":"code","source":["# ===============================\n","# 1. Install Required Libraries\n","# ===============================\n","!pip install transformers datasets evaluate peft --quiet"],"metadata":{"id":"5Ip19wyb2omy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 2. Load and Prepare Dataset\n","# ===============================\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from transformers import AutoTokenizer\n","\n","# Load and sample IMDB dataset\n","path = \"/content/drive/MyDrive/DL/project3/IMDB Dataset.csv\"\n","df = pd.read_csv(path)\n","df = df.sample(n=10000, random_state=42).reset_index(drop=True)\n","df['label'] = df['sentiment'].map({'positive': 1, 'negative': 0})\n","\n","train_texts, val_texts, train_labels, val_labels = train_test_split(\n","    df['review'].tolist(), df['label'].tolist(), test_size=0.2, random_state=42\n",")"],"metadata":{"id":"_JxxZmGJ2qe2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 3. Tokenize Texts\n","# ===============================\n","model_name = \"distilbert-base-uncased\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name)\n","train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=256)\n","val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=256)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xoh8eMsY2uvR","executionInfo":{"status":"ok","timestamp":1753636930668,"user_tz":240,"elapsed":9369,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"b33b277d-26f5-4461-8092-878197dcd071"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 4. Create Dataset Class\n","# ===============================\n","import torch\n","\n","class IMDBDataset(torch.utils.data.Dataset):\n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","    def __len__(self):\n","        return len(self.labels)\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item[\"labels\"] = torch.tensor(self.labels[idx])\n","        return item\n","\n","train_dataset = IMDBDataset(train_encodings, train_labels)\n","val_dataset = IMDBDataset(val_encodings, val_labels)"],"metadata":{"id":"VSnGzzw02vci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 5. Load Model with LoRA\n","# ===============================\n","from transformers import AutoModelForSequenceClassification\n","from peft import get_peft_model, LoraConfig, TaskType\n","\n","model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n","lora_config = LoraConfig(\n","    task_type=TaskType.SEQ_CLS,\n","    r=8,\n","    lora_alpha=32,\n","    lora_dropout=0.1,\n","    bias=\"none\",\n","    target_modules=[\"q_lin\", \"v_lin\"]\n",")\n","model = get_peft_model(model, lora_config)\n","model.print_trainable_parameters()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQjlmjgs2yBs","executionInfo":{"status":"ok","timestamp":1753636942245,"user_tz":240,"elapsed":7187,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"e4051c7b-ac07-4292-a3c3-66a0afd289ab"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["trainable params: 739,586 || all params: 67,694,596 || trainable%: 1.0925\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 6. Define Training Configuration\n","# ===============================\n","from transformers import TrainingArguments\n","\n","training_args = TrainingArguments(\n","    output_dir=\"./bert-imdb\",\n","    num_train_epochs=3,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    eval_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    logging_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    logging_dir=\"./logs\",\n","    report_to=\"none\"\n",")"],"metadata":{"id":"_-eV9mTn21Et"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 7. Define Evaluation Metrics\n","# ===============================\n","import evaluate\n","accuracy = evaluate.load(\"accuracy\")\n","f1 = evaluate.load(\"f1\")\n","\n","def compute_metrics(eval_pred):\n","    logits, labels = eval_pred\n","    preds = logits.argmax(axis=-1)\n","    return {\n","        \"accuracy\": accuracy.compute(predictions=preds, references=labels)[\"accuracy\"],\n","        \"f1\": f1.compute(predictions=preds, references=labels, average=\"weighted\")[\"f1\"]\n","    }"],"metadata":{"id":"r9v1wFLV27YC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===============================\n","# 8. Train the Model\n","# ===============================\n","from transformers import Trainer\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=train_dataset,\n","    eval_dataset=val_dataset,\n","    tokenizer=tokenizer,\n","    compute_metrics=compute_metrics\n",")\n","\n","trainer.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":276},"id":"Iufba9Mm28H_","executionInfo":{"status":"ok","timestamp":1753637400186,"user_tz":240,"elapsed":442180,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"7d6749b7-1f92-4cb0-af91-20586ec6817f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-9-1661270016.py:6: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n","  trainer = Trainer(\n","No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1500/1500 07:20, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n"," <tr style=\"text-align: left;\">\n","      <th>Epoch</th>\n","      <th>Training Loss</th>\n","      <th>Validation Loss</th>\n","      <th>Accuracy</th>\n","      <th>F1</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>1</td>\n","      <td>0.408000</td>\n","      <td>0.314374</td>\n","      <td>0.874000</td>\n","      <td>0.873665</td>\n","    </tr>\n","    <tr>\n","      <td>2</td>\n","      <td>0.303900</td>\n","      <td>0.284283</td>\n","      <td>0.882500</td>\n","      <td>0.882496</td>\n","    </tr>\n","    <tr>\n","      <td>3</td>\n","      <td>0.286400</td>\n","      <td>0.281029</td>\n","      <td>0.886000</td>\n","      <td>0.885992</td>\n","    </tr>\n","  </tbody>\n","</table><p>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["TrainOutput(global_step=1500, training_loss=0.332762685139974, metrics={'train_runtime': 441.1292, 'train_samples_per_second': 54.406, 'train_steps_per_second': 3.4, 'total_flos': 1616872882176000.0, 'train_loss': 0.332762685139974, 'epoch': 3.0})"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["# ===============================\n","# 9. Evaluate the Model\n","# ===============================\n","results = trainer.evaluate()\n","print(\"\\nðŸ“Š Final Evaluation:\")\n","for k, v in results.items():\n","    print(f\"{k}: {v:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":193},"id":"yGqvqiqc2-_K","executionInfo":{"status":"ok","timestamp":1753637418980,"user_tz":240,"elapsed":15131,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"a7b7da4b-78b5-4e6b-9920-6839c95de237"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","    <div>\n","      \n","      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [125/125 00:14]\n","    </div>\n","    "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n","ðŸ“Š Final Evaluation:\n","eval_loss: 0.2810\n","eval_accuracy: 0.8860\n","eval_f1: 0.8860\n","eval_runtime: 15.1251\n","eval_samples_per_second: 132.2310\n","eval_steps_per_second: 8.2640\n","epoch: 3.0000\n"]}]},{"cell_type":"code","source":["# ===============================\n","# 10. Predict Sample\n","# ===============================\n","def predict(text):\n","    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=256)\n","    inputs = {k: v.to(model.device) for k, v in inputs.items()}  # Move inputs to the same device as the model\n","    with torch.no_grad():\n","        outputs = model(**inputs)\n","        probs = torch.softmax(outputs.logits, dim=-1)\n","        predicted_class = torch.argmax(probs).item()\n","    return \"positive\" if predicted_class == 1 else \"negative\", probs[0].tolist()\n","\n","# Example prediction\n","print(\"\\nðŸ§ª Example Prediction:\")\n","example_text = \"i dont any word .\"\n","pred, prob = predict(example_text)\n","print(f\"Text: {example_text}\\nPrediction: {pred}, Probabilities: {prob}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TXxYcou-79Xc","executionInfo":{"status":"ok","timestamp":1753637581543,"user_tz":240,"elapsed":24,"user":{"displayName":"Kevin Nguyen","userId":"01694590780485184673"}},"outputId":"5bdd6ce2-ed47-4b0b-b283-30410f1ccfaa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","ðŸ§ª Example Prediction:\n","Text: i dont any word .\n","Prediction: negative, Probabilities: [0.7518314719200134, 0.2481684684753418]\n"]}]}],"metadata":{"language_info":{"name":"python"},"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":0}